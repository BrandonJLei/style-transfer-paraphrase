paraphrase = [
    [('module',), ['run_lm_finetuning_dynamic']],
    [('generation_module',), ['run_generation_dynamic']],
    [('model_name',), ['gpt2-large']],
    [('dataset',), ["paranmt/filter_and_kt_less_precision_less_lendiff_less_0.0_0.5_5_english_only"]],
    [('roberta_dir',), ['$ROBERTA_BASE']],
    [('content_aggregation',), [1000]],
    [('roberta_layer',), [10]],
    [('content_aggregation_type',), ["bow"]],
    [('roberta_ckpt_file',), ['model.pt']],
    [('batch_size',), [5]],
    [('accumulation',), [2]],
    [('num_epochs',), [3]],
    [('beam_size',), [1]],
    [('eval_batch_size',), [1]],
    [('context_type',), ["content"]],
    [('extra_embedding_dim',), [768]],
    [('roberta_weights',), ["fixed"]],
    [('switch_type',), ["constant"]],
    [('learning_rate',), ["5e-5,5e-5"]],
    [('generator_loss_constants',), ["1,0"]],
    [('gpu',), ["m40"]],
    [('ngpus',), ["1"]],
    [('context_input_type',), ["paraphrase"]],
    [('roberta_input_type',), ["nofilter"]],
    [('context_noise',), ["none"]],
    [('global_dense_feature_list',), ["none"]],
    [('stop_token',), ["eos"]],
    [('save_steps',), [500]],
    [('save_total_limit',), [-1]],
    [('specific_author_train',), [-1]],
    [('optimizer',), ["adam"]]
]

inverse_paraphrase = [
    [('module',), ['run_lm_finetuning_dynamic']],
    [('generation_module',), ['run_generation_dynamic']],
    [('model_name',), ['gpt2-large']],
    [('dataset',), ["shakespeare/unsupervised_prior_detokenize"]],
    [('roberta_dir',), ['$ROBERTA_BASE']],
    [('content_aggregation',), [1000]],
    [('roberta_layer',), [10]],
    [('content_aggregation_type',), ["bow"]],
    [('roberta_ckpt_file',), ['model.pt']],
    [('batch_size',), [5]],
    [('accumulation',), [2]],
    [('num_epochs',), [3]],
    [('beam_size',), [1]],
    [('eval_batch_size',), [1]],
    [('context_type',), ["content"]],
    [('extra_embedding_dim',), [768]],
    [('roberta_weights',), ["fixed"]],
    [('switch_type',), ["constant"]],
    [('learning_rate',), ["5e-5,5e-5"]],
    [('generator_loss_constants',), ["1,0"]],
    [('gpu',), ["m40"]],
    [('ngpus',), ["1"]],
    [('context_input_type',), ["roberta_input"]],
    [('roberta_input_type',), ["paraphrase_250"]],
    [('context_noise',), ["none"]],
    [('global_dense_feature_list',), ["none"]],
    [('stop_token',), ["eos"]],
    [('specific_author_train',), [0, 1]],
    [('save_steps',), [500]],
    [('save_total_limit',), [-1]],
    [('optimizer',), ["adam"]]
]
