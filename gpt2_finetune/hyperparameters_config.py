roberta_disentangle_dir = "/mnt/nfs/work1/miyyer/kalpesh/projects/style-embeddings/source-separator/saved_models/"
roberta_base_dir = "/mnt/nfs/work1/miyyer/kalpesh/projects/style-embeddings/author_data/"
paranmt_dir = "/mnt/nfs/work1/miyyer/datasets/paranmt"
simplewiki_dir = "/mnt/nfs/work1/miyyer/datasets/simplewiki"
wikilarge_dir = "/mnt/nfs/work1/miyyer/datasets/wikilarge"
style_base_dir = "/mnt/nfs/work1/miyyer/kalpesh/projects/style-embeddings/author_data/authors_1M_tokens_37_classes_srl_arg0_arg1"
newsela_dir = "/mnt/nfs/work1/miyyer/kalpesh/projects/style-embeddings/newsela/newsela_sentences"
shakespeare_supervised_dir = "/mnt/nfs/work1/miyyer/kalpesh/projects/style-embeddings/shakespeare/supervised"
shakespeare_supervised_dir_filtered = "/mnt/nfs/work1/miyyer/kalpesh/projects/style-embeddings/shakespeare/supervised_filtered"
shakespeare_unsupervised_dir_filtered = "/mnt/nfs/work1/miyyer/kalpesh/projects/style-embeddings/shakespeare/unsupervised_filtered"
shakespeare_aae_tweets_dir = "/mnt/nfs/work1/miyyer/kalpesh/projects/style-embeddings/dataset_pools/shakespeare_aae_tweets"
seven_styles_dir = "/mnt/nfs/work1/miyyer/kalpesh/projects/style-embeddings/dataset_pools/shakespeare_aae_tweets_bible_romantic-poetry_joyce_congress-bills"
six_styles_dir = "/mnt/nfs/work1/miyyer/kalpesh/projects/style-embeddings/dataset_pools/shakespeare_aae_tweets_bible_romantic-poetry_switchboard"


style_transfer = [
    [('module',), ['run_lm_finetuning_dynamic']],
    [('generation_module',), ['run_generation_dynamic']],
    [('model_name',), ['gpt2']],
    [('dataset',), ["/mnt/nfs/work1/miyyer/kalpesh/projects/style-embeddings/author_data/authors_1M_tokens_37_classes_srl_arg0_arg1"]],
    [('roberta_dir',), ['$ROBERTA_BASE']],
    [('content_aggregation',), [1000]],
    [('roberta_layer',), [10]],
    [('content_aggregation_type',), ["bow"]],
    [('roberta_ckpt_file',), ['model.pt']],
    [('batch_size',), [1]],
    [('accumulation',), [24]],
    [('num_epochs',), [15]],
    [('beam_size',), [1]],
    [('eval_batch_size',), [1]],
    [('context_type',), ["content"]],
    [('extra_embedding_dim',), [768]],
    [('roberta_weights',), ["fixed"]],
    [('switch_type',), ["constant"]],
    [('learning_rate',), ["5e-5,5e-5"]],
    [('generator_loss_constants',), ["1,0"]],
    [('gpu',), ["1080ti"]],
    [('ngpus',), ["8"]],
    [('context_input_type',), ["no_srl_input"]],
    [('roberta_input_type',), ["paraphrase_126"]],
    [('context_noise',), ["none"]],
    [('global_dense_feature_list',), ["original,pos_tags,punctuation,shuffle", "none"]],
    [('stop_token',), ["eos"]],
    [('specific_author_train',), [-1]],
    [('save_steps',), [1000]],
    [('save_total_limit',), [5]],
    [('optimizer',), ["adam"]]
]

para_paraphrase = [
    [('module',), ['run_lm_finetuning_dynamic']],
    [('generation_module',), ['run_generation_dynamic']],
    [('model_name',), ['gpt2']],
    [('dataset',), ["/mnt/nfs/work1/miyyer/kalpesh/projects/style-embeddings/author_data/authors_1M_tokens_37_classes_srl_arg0_arg1"]],
    [('roberta_dir',), ['$ROBERTA_BASE']],
    [('content_aggregation',), [1000]],
    [('roberta_layer',), [10]],
    [('content_aggregation_type',), ["bow"]],
    [('roberta_ckpt_file',), ['model.pt']],
    [('batch_size',), [1]],
    [('accumulation',), [12]],
    [('num_epochs',), [10]],
    [('beam_size',), [1]],
    [('eval_batch_size',), [1]],
    [('context_type',), ["content"]],
    [('extra_embedding_dim',), [768]],
    [('roberta_weights',), ["fixed"]],
    [('switch_type',), ["constant"]],
    [('learning_rate',), ["5e-5,5e-5"]],
    [('generator_loss_constants',), ["1,0"]],
    [('gpu',), ["1080ti"]],
    [('ngpus',), ["8"]],
    [('context_input_type',), ["roberta_input"]],
    [('roberta_input_type',), ["paraphrase_219_shuffle_sentences"]],
    [('context_noise',), ["none"]],
    [('global_dense_feature_list',), ["none"]],
    [('stop_token',), ["eos"]],
    [('specific_author_train',), [-1]],
    [('save_steps',), [1000]],
    [('save_total_limit',), [-1]],
    [('optimizer',), ["adam"]]
]

style_transfer_sentence = [
    [('module',), ['run_lm_finetuning_dynamic']],
    [('generation_module',), ['run_generation_dynamic']],
    [('model_name',), ['gpt2']],
    [('dataset',), ["{}_single_sentence".format(style_base_dir)]],
    [('roberta_dir',), ['$ROBERTA_BASE']],
    [('content_aggregation',), [1000]],
    [('roberta_layer',), [10]],
    [('content_aggregation_type',), ["bow"]],
    [('roberta_ckpt_file',), ['model.pt']],
    [('batch_size',), [10]],
    [('accumulation',), [24]],
    [('num_epochs',), [10]],
    [('beam_size',), [1]],
    [('eval_batch_size',), [1]],
    [('context_type',), ["content"]],
    [('extra_embedding_dim',), [768]],
    [('roberta_weights',), ["fixed"]],
    [('switch_type',), ["constant"]],
    [('learning_rate',), ["5e-5,5e-5"]],
    [('generator_loss_constants',), ["1,0"]],
    [('gpu',), ["1080ti"]],
    [('ngpus',), ["8"]],
    [('context_input_type',), ["roberta_input"]],
    [('roberta_input_type',), ["paraphrase_126"]],
    [('context_noise',), ["none"]],
    [('global_dense_feature_list',), ["none"]],
    [('stop_token',), ["eos"]],
    [('specific_author_train',), [0]],
    [('save_steps',), [250]],
    [('save_total_limit',), [10]],
    [('optimizer',), ["adam"]]
]

paraphrase = [
    [('module',), ['run_lm_finetuning_dynamic']],
    [('generation_module',), ['run_generation_dynamic']],
    [('model_name',), ['gpt2-medium']],
    [('dataset',), [paranmt_dir + "/filter_and_kt_less_precision_less_lendiff_less_0.0_0.5_5_english_only"]],
    [('roberta_dir',), ['$ROBERTA_BASE']],
    [('content_aggregation',), [1000]],
    [('roberta_layer',), [10]],
    [('content_aggregation_type',), ["bow"]],
    [('roberta_ckpt_file',), ['model.pt']],
    [('batch_size',), [5]],
    [('accumulation',), [2]],
    [('num_epochs',), [3]],
    [('beam_size',), [1]],
    [('eval_batch_size',), [1]],
    [('context_type',), ["content"]],
    [('extra_embedding_dim',), [768]],
    [('roberta_weights',), ["fixed"]],
    [('switch_type',), ["constant"]],
    [('learning_rate',), ["5e-5,5e-5"]],
    [('generator_loss_constants',), ["1,0"]],
    [('gpu',), ["m40"]],
    [('ngpus',), ["1"]],
    [('context_input_type',), ["paraphrase"]],
    [('roberta_input_type',), ["nofilter"]],
    [('context_noise',), ["none"]],
    [('global_dense_feature_list',), ["none"]],
    [('stop_token',), ["eos"]],
    [('save_steps',), [500]],
    [('save_total_limit',), [-1]],
    [('specific_author_train',), [-1]],
    [('optimizer',), ["adam"]]
]

shakespeare = [
    [('module',), ['run_lm_finetuning_dynamic']],
    [('generation_module',), ['run_generation_dynamic']],
    [('model_name',), ['gpt2-medium']],
    [('dataset',), [shakespeare_supervised_dir_filtered]],
    [('roberta_dir',), ['$ROBERTA_BASE']],
    [('content_aggregation',), [1000]],
    [('roberta_layer',), [10]],
    [('content_aggregation_type',), ["bow"]],
    [('roberta_ckpt_file',), ['model.pt']],
    [('batch_size',), [5]],
    [('accumulation',), [1]],
    [('num_epochs',), [10]],
    [('beam_size',), [1]],
    [('eval_batch_size',), [1]],
    [('context_type',), ["content"]],
    [('extra_embedding_dim',), [768]],
    [('roberta_weights',), ["fixed"]],
    [('switch_type',), ["constant"]],
    [('learning_rate',), ["5e-5,5e-5"]],
    [('generator_loss_constants',), ["1,0"]],
    [('gpu',), ["1080ti"]],
    [('ngpus',), ["8"]],
    [('context_input_type',), ["paraphrase"]],
    [('roberta_input_type',), ["nofilter"]],
    [('context_noise',), ["none"]],
    [('global_dense_feature_list',), ["none"]],
    [('stop_token',), ["eos"]],
    [('save_steps',), [250]],
    [('save_total_limit',), [-1]],
    [('specific_author_train',), [-1]],
    [('optimizer',), ["adam"]]
]

shakespeare_unsupervised = [
    [('module',), ['run_lm_finetuning_dynamic']],
    [('generation_module',), ['run_generation_dynamic']],
    [('model_name',), ['gpt2-medium']],
    [('dataset',), [shakespeare_unsupervised_dir_filtered]],
    [('roberta_dir',), ['$ROBERTA_BASE']],
    [('content_aggregation',), [1000]],
    [('roberta_layer',), [10]],
    [('content_aggregation_type',), ["bow"]],
    [('roberta_ckpt_file',), ['model.pt']],
    [('batch_size',), [5]],
    [('accumulation',), [1]],
    [('num_epochs',), [3]],
    [('beam_size',), [1]],
    [('eval_batch_size',), [1]],
    [('context_type',), ["content"]],
    [('extra_embedding_dim',), [768]],
    [('roberta_weights',), ["fixed"]],
    [('switch_type',), ["constant"]],
    [('learning_rate',), ["5e-5,5e-5"]],
    [('generator_loss_constants',), ["1,0"]],
    [('gpu',), ["1080ti"]],
    [('ngpus',), ["8"]],
    [('context_input_type',), ["roberta_input", "no_srl_input"]],
    [('roberta_input_type',), ["paraphrase_219"]],
    [('context_noise',), ["none"]],
    [('global_dense_feature_list',), ["original", "none"]],
    [('stop_token',), ["eos"]],
    [('specific_author_train',), [-1]],
    [('save_steps',), [250]],
    [('save_total_limit',), [-1]],
    [('optimizer',), ["adam"]]
]

new_style_dataset = [
    [('module',), ['run_lm_finetuning_dynamic']],
    [('generation_module',), ['run_generation_dynamic']],
    [('model_name',), ['gpt2-medium']],
    [('dataset',), ["/mnt/nfs/work1/miyyer/kalpesh/projects/style-embeddings/shakespeare/unsupervised_prior_detokenize"]],
    [('roberta_dir',), ['$ROBERTA_BASE']],
    [('content_aggregation',), [1000]],
    [('roberta_layer',), [10]],
    [('content_aggregation_type',), ["bow"]],
    [('roberta_ckpt_file',), ['model.pt']],
    [('batch_size',), [5]],
    [('accumulation',), [2]],
    [('num_epochs',), [3]],
    [('beam_size',), [1]],
    [('eval_batch_size',), [1]],
    [('context_type',), ["content"]],
    [('extra_embedding_dim',), [768]],
    [('roberta_weights',), ["fixed"]],
    [('switch_type',), ["constant"]],
    [('learning_rate',), ["5e-5,5e-5"]],
    [('generator_loss_constants',), ["1,0"]],
    [('gpu',), ["m40"]],
    [('ngpus',), ["1"]],
    [('context_input_type',), ["roberta_input"]],
    [('roberta_input_type',), ["paraphrase_362"]],
    [('context_noise',), ["none"]],
    [('global_dense_feature_list',), ["none"]],
    [('stop_token',), ["eos"]],
    [('specific_author_train',), [0, 1]],
    [('save_steps',), [500]],
    [('save_total_limit',), [-1]],
    [('optimizer',), ["adam"]]
]

newsela = [
    [('module',), ['run_lm_finetuning_dynamic']],
    [('generation_module',), ['run_generation_dynamic']],
    [('model_name',), ['gpt2']],
    [('dataset',), [newsela_dir]],
    [('roberta_dir',), ['$ROBERTA_BASE']],
    [('content_aggregation',), [1000]],
    [('roberta_layer',), [10]],
    [('content_aggregation_type',), ["bow"]],
    [('roberta_ckpt_file',), ['model.pt']],
    [('batch_size',), [4]],
    [('accumulation',), [6]],
    [('num_epochs',), [5, 10]],
    [('beam_size',), [1]],
    [('eval_batch_size',), [1]],
    [('context_type',), ["content"]],
    [('extra_embedding_dim',), [768]],
    [('roberta_weights',), ["fixed"]],
    [('switch_type',), ["constant"]],
    [('learning_rate',), ["5e-5,5e-5"]],
    [('generator_loss_constants',), ["1,0"]],
    [('gpu',), ["1080ti"]],
    [('ngpus',), ["8"]],
    [('context_input_type',), ["paraphrase"]],
    [('roberta_input_type',), ["nofilter"]],
    [('context_noise',), ["none"]],
    [('global_dense_feature_list',), ["none"]],
    [('stop_token',), ["eos"]],
    [('save_steps',), [1000]],
    [('save_total_limit',), [5]],
    [('specific_author_train',), [-1]],
    [('optimizer',), ["adam"]]
]

simplewiki = [
    [('module',), ['run_lm_finetuning_dynamic']],
    [('generation_module',), ['run_generation_dynamic']],
    [('model_name',), ['gpt2']],
    [('dataset',), [simplewiki_dir, wikilarge_dir + "/precision_less_0.5"]],
    [('roberta_dir',), ['$ROBERTA_BASE']],
    [('content_aggregation',), [1000]],
    [('roberta_layer',), [10]],
    [('content_aggregation_type',), ["bow"]],
    [('roberta_ckpt_file',), ['model.pt']],
    [('batch_size',), [4]],
    [('accumulation',), [6]],
    [('num_epochs',), [10]],
    [('beam_size',), [1]],
    [('eval_batch_size',), [1]],
    [('context_type',), ["content"]],
    [('extra_embedding_dim',), [768]],
    [('roberta_weights',), ["fixed"]],
    [('switch_type',), ["constant"]],
    [('learning_rate',), ["5e-5,5e-5"]],
    [('generator_loss_constants',), ["1,0"]],
    [('gpu',), ["1080ti"]],
    [('ngpus',), ["8"]],
    [('context_input_type',), ["simplewiki"]],
    [('roberta_input_type',), ["nofilter"]],
    [('context_noise',), ["none"]],
    [('global_dense_feature_list',), ["none"]],
    [('stop_token',), ["eos"]],
    [('save_steps',), [1000]],
    [('save_total_limit',), [10]],
    [('specific_author_train',), [-1]],
    [('optimizer',), ["adam"]]
]
